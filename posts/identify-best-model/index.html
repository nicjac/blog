<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Good practices for neural network training: identify, save, and document best models | Reasonable AI</title>
<meta name=keywords content="Deep Learning,AI,Weights&Biases,fastai,good practices">
<meta name=description content="An introduction to the good practice of best model saving when training a neural network, and a practical implementation using fastai and Weights & Biases">
<meta name=author content="Nicolas Jaccard">
<link rel=canonical href=https://nicjac.dev/posts/identify-best-model/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nicjac.dev/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://nicjac.dev/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://nicjac.dev/favicon-32x32.png>
<link rel=apple-touch-icon href=https://nicjac.dev/apple-touch-icon.png>
<link rel=mask-icon href=https://nicjac.dev/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><script defer data-domain=nicjac.dev src=https://plausible.io/js/plausible.js></script><meta property="og:title" content="Good practices for neural network training: identify, save, and document best models">
<meta property="og:description" content="An introduction to the good practice of best model saving when training a neural network, and a practical implementation using fastai and Weights & Biases">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nicjac.dev/posts/identify-best-model/"><meta property="og:image" content="https://nicjac.dev/images/training/overfitting-best-model.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-30T10:00:08+00:00">
<meta property="article:modified_time" content="2021-12-30T10:00:08+00:00"><meta property="og:site_name" content="Reasonable AI">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nicjac.dev/images/training/overfitting-best-model.png">
<meta name=twitter:title content="Good practices for neural network training: identify, save, and document best models">
<meta name=twitter:description content="An introduction to the good practice of best model saving when training a neural network, and a practical implementation using fastai and Weights & Biases">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nicjac.dev/posts/"},{"@type":"ListItem","position":2,"name":"Good practices for neural network training: identify, save, and document best models","item":"https://nicjac.dev/posts/identify-best-model/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Good practices for neural network training: identify, save, and document best models","name":"Good practices for neural network training: identify, save, and document best models","description":"An introduction to the good practice of best model saving when training a neural network, and a practical implementation using fastai and Weights \u0026amp; Biases\n","keywords":["Deep Learning","AI","Weights\u0026Biases","fastai","good practices"],"articleBody":"An introduction to the good practice of best model saving when training a neural network, and a practical implementation using fastai and Weights \u0026 Biases\nPreamble In this post, we are going to:\n Introduce the concept of best model Discuss how the best model can be identified, saved, and documented during training Explore how to leverage fastai and Weights \u0026 Biases to do all of this (almost) automatically and effortlessly  What is a best model and why should I care? Model training can be seen as the generation of subsequent versions of a model – after each batch, the model weights are adjusted, and as a result, a new version of the model is created. Each new version will have varying levels of performance (as evaluated against a validation set).\nIf everything goes well, training and validation loss will decrease with the number of training epochs. However, the best performing version of a model (here abbreviated as best model) is rarely the one obtained at the end of the training process.\n A somewhat typical example of overfitting training curves.\n  Take a typical overfitting case – at first, both training and validation losses decrease as training progresses. At some point, the validation loss might start increasing, even though the training loss continues to decrease; from this point on, subsequent model versions produced during the training process are overfitting the training data. These model versions are less likely to generalize well to unseen data. In this case, the best model would be the one obtained at the point where the validation loss started to diverge.\nOverfitting is a convenient example, but similar observations would also apply to other dynamics of model training, such as the presence of local maxima or minima.\nIdentifying and saving the best model during training The naive approach to the problem would be to save our model after every epoch during training and to retrospectively select the optimal version based on the training curves. This approach is associated with a couple of notable drawbacks:\n Storage space: large models tend to occupy a significant amount of storage when saved, with file sizes reaching 100s of MBs to GBs. Multiply this by the number of epochs, and you end up with a pretty sizeable amount of storage dedicated to saving all versions of a model. This can quickly become problematic, especially when training models remotely. Computational impact: saving a model after every epoch will impact overall training time – serialization / export of some models can be computationally intensive and slow.   Naive approach where all model versions are persisted versus the best model only approach, where only the most recent best performing model is persisted\n  As an alternative to this brute force approach where all model versions are saved during training, one can be more selective. We know from the above that our best model is likely one associated with a low validation loss. We can thus formulate a criterion for the selection of our best model: it must have a lower validation loss than the previous candidate. As pseudo-code:\nif current validation loss lower than candidate validation loss: save model to disk overwriting previous candidate set candidate validation loss to current validation loss The key advantages of this approach is that a) a new model is exported only when the validation loss is improved over the previous best candidate, and b) we only ever have one model version persisted to storage at any given time. As thus, we successfully addressed the two drawbacks of the naive approach.\nMaybe more importantly, only saving the best model also encourages good practices by requiring the performance evaluation methodology to be decided before training starts, and it removes the temptation to retroactively evaluate other versions of the model on a separate test dataset.\nA note on validation loss, alternative metrics, and model documentation Up to this point, we used validation loss as our target metric to identify the best model during training. Why validation loss you might ask? The fact that it is almost always computed during training made it a convenient example to illustrate the concepts discussed in this article. However, validation loss might not be that relevant to your particular use-case or domain, and any other metric can be used instead. For classification tasks, accuracy could be a good choice. Similarly, you can choose the target metric to ensure that the best model is also one that will generalize well to unseen data, for example by using Matthews Correlation Coefficient when dealing with severely imbalanced datasets.\nWhatever target metric you decide to use, it is also important to document other aspects of this particular version of the model. Typically, this would include all performance metrics tracked during training. Persisting this information together with the actual model artifact can be very useful later on, for example to rank models obtained from a hyperparameter search or to carry out integration testing when deploying in production (more on this in a future article!).\nEffortlessly save the best model during training with fastai Implementation of best model saving requires alteration of the training loop in order to monitor the target metric and to trigger model saving when an improvement is detected. Many modern frameworks come with this capability built-in. Here, we will be focusing on a fastai implementation, but similar capabilities are likely available for your library of choice. You can still follow along to get an idea how this can be implemented in practice.\nIf you are unaware of what fastai is, its official description is:\n fastai simplifies training fast and accurate neural nets using modern best practices\n The fastai training loop can be modified and extended using callback methods that are called at specific stages of training, for example after an epoch is completed, or at the end of training. Conveniently, the SaveModelCallback happens to do (almost) exactly what we need.\nUsing the callback couldn’t be easier:\nlearner.fit_one_cycle(... ,cbs=[..., SaveModelCallback(monitor='valid_loss')]) where learner is a standard fastai Learner object. By default, the callback will track the validation loss to determine when to save a new best model. Use the monitor argument to set it to any other metric tracked by your learner object. Following each epoch during training, the current value for the target metric is compared to the previous best value - if it is an improvement, the model is persisted in the models directory (and overwriting the previous best candidate, if present).\nBehind the scene, the callback tries to figure whether an improvement is a smaller value (if the target metric contains loss or error) or a larger value (everything else). This behavior can be overridden using the comp argument. The model is persisted using fastai’s save_model function, which is a wrapper for Pytorch’s native torch.save.\nThe reason why the built-in callback is not exactly what we need is that it will only log the target metric used to identify the best model, and nothing else. It will not log other metrics (for example accuracy, if the best model is determined based on validation loss). This might be fine, but given that our best model might end up being used as part of a product somewhere, it would be a good idea to characterize it as much as possible. I put together a custom version of the SaveModelCallback that will log all metrics tracked by fastai during training. The code for can be found here.\nThis custom version of the callback can be used as a drop-in replacement. All it really does is to internally keep track of a dictionary of metrics (last_saved_metadata) associated with the best model. How to make use of this? All is to be revealed in the next section!\nAutomatically document the best model with Weights \u0026 Biases Saving the best model locally is a good start, but it can quickly become unwieldy if you work remotely, or carry out large number of experiments. So how to keep track of the models created, and of their associated metrics? This is where Weights \u0026 Biases comes in. W\u0026B is one of those tools that make you wonder how you have ever been able to properly function without them. While officially described as “The developer-first MLOps platform”, I prefer to refer to it as the swiss army knife of MLOps.\nW\u0026B is very useful to track and compare experiments. However, for the purpose of this article, we are mainly interested in its almost universal versioning capabilities. In the W\u0026B ecosystem, artifacts are components that can be versioned, possibly together with their lineage. Models can be versioned as artifacts.\nConveniently, fastai has a built-in callback to integrate with W\u0026B, aptly named WandbCallback. To use it, one need to initialize a W\u0026B run, and to add the callback to the learner object like so:\n# Import W\u0026B package import wandb # Initialize W\u0026B run (can potentially set project name, run name, etc...) wandb.init() # Add Callback to learner to track training metrics and log best models learn = learner(..., cbs=WandbCallback()) The main purpose of the callback is to log useful telemetry regarding the training process to your W\u0026B account, including environment information and metrics. The magic happens when it is used in combination with the SaveModelCallback – at the end of the training process, the best performing model will be automatically logged as an artifact of the W\u0026B run.\nThere is one major issue with the default WandbCallback: the metadata associated with the model is recorded at the end of the run and not at the epoch when the best model was saved. In other words, the metadata does not correspond to the saved model at all, and can be misleading (for example when the tracked metric diverged towards the end of training due to overfitting).\nThis is where the custom SaveModelCallback that was discussed in the previous section comes in. It will save all the information needed to associate the model with its actual metadata. To take advantage of this, it is also necessary to use a custom version of WandbCallback, which can be found here.\nThe changes made in the custom callback are highlighted here:\n101 102 103 104 105 106 107 108 109  def after_fit(self): if self.log_model: if self.save_model.last_saved_path is None: print('WandbCallback could not retrieve a model to upload') else: log_model(self.save_model.last_saved_path, metadata=self.save_model.last_saved_metadata) for metadata_key in self.save_model.last_saved_metadata: wandb.run.summary[f'best_{metadata_key}'] = self.save_model.last_saved_metadata[metadata_key]   As a result, the following will automatically happen:\n The model logged to the W\u0026B run is associated with metadata containing the correct metric values The values for all metrics for the best model are added to the run summary, with the prefix best_. This allows runs to be sorted and compared based on the performance of their respective best model   Best model logged in Weights and Biases. (Left) Model metadata including key metrics; (Right) Models in a W\u0026B project sorted by the best_matthews_corrcoef metadata associated with their respective best models\n  Wrapping up So, what have we learned in this article?\n Only saving the best model during training is efficient and encourages good practices The metadata, including key metrics associated with the best model, is almost as important as the model artifact itself Using fastai and Weights \u0026 Biases, saving and documenting the best model can be done automatically for you. Two custom callback functions were described to make this process even better (SaveModelCallback and WandbCallback). ","wordCount":"1885","inLanguage":"en","datePublished":"2021-12-30T10:00:08Z","dateModified":"2021-12-30T10:00:08Z","author":{"@type":"Person","name":"Nicolas Jaccard"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nicjac.dev/posts/identify-best-model/"},"publisher":{"@type":"Organization","name":"Reasonable AI","logo":{"@type":"ImageObject","url":"https://nicjac.dev/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://nicjac.dev/ accesskey=h title="Reasonable AI (Alt + H)">Reasonable AI</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://nicjac.dev/about/ title="About Me">
<span>About Me</span>
</a>
</li>
<li>
<a href=https://nicjac.dev/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=https://nicjac.dev/categories/ title=Categories>
<span>Categories</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://nicjac.dev/>Home</a>&nbsp;»&nbsp;<a href=https://nicjac.dev/posts/>Posts</a></div>
<h1 class=post-title>
Good practices for neural network training: identify, save, and document best models
</h1>
<div class=post-meta><span title="2021-12-30 10:00:08 +0000 UTC">30 December, 2021</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Nicolas Jaccard
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#preamble aria-label=Preamble>Preamble</a></li>
<li>
<a href=#what-is-a-_best-model_-and-why-should-icare aria-label="What is a best model and why should I care?">What is a <em>best model</em> and why should I care?</a></li>
<li>
<a href=#identifying-and-saving-the-best-model-duringtraining aria-label="Identifying and saving the best model during training">Identifying and saving the <em>best model</em> during training</a></li>
<li>
<a href=#a-note-on-validation-loss-alternative-metrics-and-model-documentation aria-label="A note on validation loss, alternative metrics, and model documentation">A note on validation loss, alternative metrics, and model documentation</a></li>
<li>
<a href=#effortlessly-save-the-best-model-during-training-with-fastai aria-label="Effortlessly save the best model during training with fastai">Effortlessly save the <em>best model</em> during training with fastai</a></li>
<li>
<a href=#automatically-document-the-best-model-with-weights--biases aria-label="Automatically document the best model with Weights &amp;amp; Biases">Automatically document the <em>best model</em> with Weights & Biases</a></li>
<li>
<a href=#wrapping-up aria-label="Wrapping up">Wrapping up</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>An introduction to the good practice of <em>best model</em> saving when training a neural network, and a practical implementation using fastai and Weights & Biases</p>
<h2 id=preamble>Preamble<a hidden class=anchor aria-hidden=true href=#preamble>#</a></h2>
<p>In this post, we are going to:</p>
<ul>
<li>Introduce the concept of <em>best model</em></li>
<li>Discuss how the <em>best model</em> can be identified, saved, and documented during training</li>
<li>Explore how to leverage fastai and Weights & Biases to do all of this (almost) automatically and effortlessly</li>
</ul>
<h2 id=what-is-a-_best-model_-and-why-should-icare>What is a <em>best model</em> and why should I care?<a hidden class=anchor aria-hidden=true href=#what-is-a-_best-model_-and-why-should-icare>#</a></h2>
<p>Model training can be seen as the generation of subsequent versions of a model  &ndash;  after each batch, the model weights are adjusted, and as a result, a new version of the model is created. Each new version will have varying levels of performance (as evaluated against a validation set).</p>
<p>If everything goes well, training and validation loss will decrease with the number of training epochs. However, the best performing version of a model (here abbreviated as <em>best model</em>) is rarely the one obtained at the end of the training process.</p>
<figure class=align-center>
<img loading=lazy src=/images/training/overfitting-best-model.png#center alt="A somewhat typical example of overfitting training curves."> <figcaption>
<p>A somewhat typical example of overfitting training curves.</p>
</figcaption>
</figure>
<p>Take a typical overfitting case  &ndash;  at first, both training and validation losses decrease as training progresses. At some point, the validation loss might start increasing, even though the training loss continues to decrease; from this point on, subsequent model versions produced during the training process are overfitting the training data. These model versions are less likely to generalize well to unseen data. In this case, the <em>best model</em> would be the one obtained at the point where the validation loss started to diverge.</p>
<p>Overfitting is a convenient example, but similar observations would also apply to other dynamics of model training, such as the presence of local maxima or minima.</p>
<h2 id=identifying-and-saving-the-best-model-duringtraining>Identifying and saving the <em>best model</em> during training<a hidden class=anchor aria-hidden=true href=#identifying-and-saving-the-best-model-duringtraining>#</a></h2>
<p>The naive approach to the problem would be to save our model after every epoch during training and to retrospectively select the optimal version based on the training curves. This approach is associated with a couple of notable drawbacks:</p>
<ul>
<li><strong>Storage space</strong>: large models tend to occupy a significant amount of storage when saved, with file sizes reaching 100s of MBs to GBs. Multiply this by the number of epochs, and you end up with a pretty sizeable amount of storage dedicated to saving all versions of a model. This can quickly become problematic, especially when training models remotely.</li>
<li><strong>Computational impact</strong>: saving a model after every epoch will impact overall training time  &ndash;  serialization / export of some models can be computationally intensive and slow.</li>
</ul>
<figure class=align-center>
<img loading=lazy src=/images/training/best-model-naive-and-best.png#center alt="Naive approach where all model versions are persisted versus the best model only approach, where only the most recent best performing model is persisted"> <figcaption>
<p>Naive approach where all model versions are persisted versus the best model only approach, where only the most recent best performing model is persisted</p>
</figcaption>
</figure>
<p>As an alternative to this brute force approach where all model versions are saved during training, one can be more selective. We know from the above that our <em>best model</em> is likely one associated with a low validation loss. We can thus formulate a criterion for the selection of our <em>best model</em>: it must have a lower validation loss than the previous candidate. As pseudo-code:</p>
<pre tabindex=0><code class=language-none data-lang=none>if current validation loss lower than candidate validation loss:
    save model to disk overwriting previous candidate
    set candidate validation loss to current validation loss
</code></pre><p>The key advantages of this approach is that a) a new model is exported only when the validation loss is improved over the previous best candidate, and b) we only ever have one model version persisted to storage at any given time. As thus, we successfully addressed the two drawbacks of the naive approach.</p>
<p>Maybe more importantly, only saving the <em>best model</em> also encourages good practices by requiring the performance evaluation methodology to be decided before training starts, and it removes the temptation to retroactively evaluate other versions of the model on a separate test dataset.</p>
<h2 id=a-note-on-validation-loss-alternative-metrics-and-model-documentation>A note on validation loss, alternative metrics, and model documentation<a hidden class=anchor aria-hidden=true href=#a-note-on-validation-loss-alternative-metrics-and-model-documentation>#</a></h2>
<p>Up to this point, we used validation loss as our target metric to identify the <em>best model</em> during training. Why validation loss you might ask? The fact that it is almost always computed during training made it a convenient example to illustrate the concepts discussed in this article. </p>
<p>However, validation loss might not be that relevant to your particular use-case or domain, and any other metric can be used instead. For classification tasks, accuracy could be a good choice. Similarly, you can choose the target metric to ensure that the <em>best model</em> is also one that will generalize well to unseen data, for example by using <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html>Matthews Correlation Coefficient</a> when dealing with severely imbalanced datasets.</p>
<p>Whatever target metric you decide to use, it is also important to document other aspects of this particular version of the model. Typically, this would include all performance metrics tracked during training. Persisting this information together with the actual model artifact can be very useful later on, for example to rank models obtained from a hyperparameter search or to carry out integration testing when deploying in production (more on this in a future article!).</p>
<h2 id=effortlessly-save-the-best-model-during-training-with-fastai>Effortlessly save the <em>best model</em> during training with fastai<a hidden class=anchor aria-hidden=true href=#effortlessly-save-the-best-model-during-training-with-fastai>#</a></h2>
<p>Implementation of best model saving requires alteration of the training loop in order to monitor the target metric and to trigger model saving when an improvement is detected. Many modern frameworks come with this capability built-in. Here, we will be focusing on a fastai implementation, but similar capabilities are likely available for your library of choice. You can still follow along to get an idea how this can be implemented in practice.</p>
<p>If you are unaware of what fastai is, its <a href=https://github.com/fastai/fastai>official description is</a>:</p>
<blockquote>
<p>fastai simplifies training fast and accurate neural nets using modern best practices</p>
</blockquote>
<p>The fastai training loop can be modified and extended using <a href=https://docs.fast.ai/callback.core.html>callback methods</a> that are called at specific stages of training, for example after an epoch is completed, or at the end of training. Conveniently, the <a href=https://docs.fast.ai/callback.tracker.html#SaveModelCallback>SaveModelCallback</a> happens to do (almost) exactly what we need.</p>
<p>Using the callback couldn&rsquo;t be easier:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>learner<span style=color:#f92672>.</span>fit_one_cycle(<span style=color:#f92672>...</span> ,cbs<span style=color:#f92672>=</span>[<span style=color:#f92672>...</span>, SaveModelCallback(monitor<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;valid_loss&#39;</span>)])
</code></pre></div><p>where <code>learner</code> is a standard fastai <a href=https://docs.fast.ai/learner.html#Learner>Learner object</a>. By default, the callback will track the validation loss to determine when to save a new <em>best model</em>. Use the <code>monitor</code> argument to set it to any other metric tracked by your <code>learner</code> object. Following each epoch during training, the current value for the target metric is compared to the previous best value - if it is an improvement, the model is persisted in the <code>models</code> directory (and overwriting the previous best candidate, if present).</p>
<p>Behind the scene, the callback tries to figure whether an improvement is a smaller value (if the target metric contains <code>loss</code> or <code>error</code>) or a larger value (everything else). This behavior can be overridden using the <code>comp</code> argument. The model is persisted using fastai&rsquo;s <a href=https://docs.fast.ai/learner.html#Learner.save><code>save_model</code></a> function, which is a wrapper for Pytorch&rsquo;s native <a href=https://pytorch.org/docs/stable/generated/torch.save.html><code>torch.save</code></a>.</p>
<p>The reason why the built-in callback is not <em>exactly</em> what we need is that it will only log the target metric used to identify the <em>best model</em>, and nothing else. It will not log other metrics (for example accuracy, if the <em>best model</em> is determined based on validation loss). This might be fine, but given that our <em>best model</em> might end up being used as part of a product somewhere, it would be a good idea to characterize it as much as possible. I put together a custom version of the <code>SaveModelCallback</code> that will log all metrics tracked by fastai during training. The code for can be found <a href=https://gist.github.com/nicjac/b363d2454ea253570a54e5e178e7666a>here</a>.</p>
<p>This custom version of the callback can be used as a drop-in replacement. All it really does is to internally keep track of a dictionary of metrics (<code>last_saved_metadata</code>) associated with the <em>best model</em>. How to make use of this? All is to be revealed in the next section!</p>
<h2 id=automatically-document-the-best-model-with-weights--biases>Automatically document the <em>best model</em> with Weights & Biases<a hidden class=anchor aria-hidden=true href=#automatically-document-the-best-model-with-weights--biases>#</a></h2>
<p>Saving the <em>best model</em> locally is a good start, but it can quickly become unwieldy if you work remotely, or carry out large number of experiments. So how to keep track of the models created, and of their associated metrics? This is where <a href=https://wandb.ai/site>Weights & Biases</a> comes in. W&B is one of those tools that make you wonder how you have ever been able to properly function without them. While officially described as &ldquo;The developer-first MLOps platform&rdquo;, I prefer to refer to it as the swiss army knife of MLOps.</p>
<p>W&B is very useful to track and compare experiments. However, for the purpose of this article, we are mainly interested in its almost universal versioning capabilities. In the W&B ecosystem, <a href=https://wandb.ai/site/artifacts>artifacts</a> are components that can be versioned, possibly together with their lineage. Models can be versioned as artifacts.</p>
<p>Conveniently, fastai has a built-in callback to integrate with W&B, aptly named <a href=https://docs.fast.ai/callback.wandb.html#WandbCallback><code>WandbCallback</code></a>. To use it, one need to initialize a W&B run, and to add the callback to the learner object like so:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Import W&amp;B package</span>
<span style=color:#f92672>import</span> wandb

<span style=color:#75715e># Initialize W&amp;B run (can potentially set project name, run name, etc...)</span>
wandb<span style=color:#f92672>.</span>init()

<span style=color:#75715e># Add Callback to learner to track training metrics and log best models</span>
learn <span style=color:#f92672>=</span> learner(<span style=color:#f92672>...</span>, cbs<span style=color:#f92672>=</span>WandbCallback())
</code></pre></div><p>The main purpose of the callback is to log useful telemetry regarding the training process to your W&B account, including environment information and metrics. The magic happens when it is used in combination with the <code>SaveModelCallback</code> &ndash; at the end of the training process, the best performing model will be automatically logged as an artifact of the W&B run.</p>
<p>There is one major issue with the default <code>WandbCallback</code>: the metadata associated with the model is recorded at the end of the run and not at the epoch when the <em>best model</em> was saved. In other words, the metadata <strong>does not</strong> correspond to the saved model at all, and can be misleading (for example when the tracked metric diverged towards the end of training due to overfitting).</p>
<p>This is where the custom <code>SaveModelCallback</code> that was discussed in the previous section comes in. It will save all the information needed to associate the model with its <em>actual</em> metadata. To take advantage of this, it is also necessary to use a custom version of <code>WandbCallback</code>, which can be found <a href=https://gist.github.com/nicjac/9efb56cccd57f9c84910f02ccabf6fac>here</a>.</p>
<p>The changes made in the custom callback are highlighted here:</p>
<div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4>
<table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0>
<pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">101
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">102
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">103
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">104
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">105
</span><span style=display:block;width:100%;background-color:#3c3d38><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">106
</span></span><span style=display:block;width:100%;background-color:#3c3d38><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">107
</span></span><span style=display:block;width:100%;background-color:#3c3d38><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">108
</span></span><span style=display:block;width:100%;background-color:#3c3d38><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">109
</span></span></code></pre></td>
<td style=vertical-align:top;padding:0;margin:0;border:0;width:100%>
<pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>after_fit</span>(self):
    <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>log_model:
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>save_model<span style=color:#f92672>.</span>last_saved_path <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
            print(<span style=color:#e6db74>&#39;WandbCallback could not retrieve a model to upload&#39;</span>)
        <span style=color:#66d9ef>else</span>:
<span style=display:block;width:100%;background-color:#3c3d38>            log_model(self<span style=color:#f92672>.</span>save_model<span style=color:#f92672>.</span>last_saved_path, metadata<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>save_model<span style=color:#f92672>.</span>last_saved_metadata)
</span><span style=display:block;width:100%;background-color:#3c3d38>            
</span><span style=display:block;width:100%;background-color:#3c3d38>            <span style=color:#66d9ef>for</span> metadata_key <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>save_model<span style=color:#f92672>.</span>last_saved_metadata:
</span><span style=display:block;width:100%;background-color:#3c3d38>                wandb<span style=color:#f92672>.</span>run<span style=color:#f92672>.</span>summary[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;best_</span><span style=color:#e6db74>{</span>metadata_key<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>save_model<span style=color:#f92672>.</span>last_saved_metadata[metadata_key]</span></code></pre></td></tr></table>
</div>
</div>
<p>As a result, the following will automatically happen:</p>
<ul>
<li>The model logged to the W&B run is associated with metadata containing the correct metric values</li>
<li>The values for all metrics for the <em>best model</em> are added to the run summary, with the prefix <code>best_</code>. This allows runs to be sorted and compared based on the performance of their respective <em>best model</em></li>
</ul>
<figure class=align-center>
<img loading=lazy src=/images/training/wandb-model-combined.png#center alt="Best model logged in Weights and Biases. (Left) Model metadata including key metrics; (Right) Models in a W&amp;amp;B project sorted by the best_matthews_corrcoef metadata associated with their respective best models"> <figcaption>
<p>Best model logged in Weights and Biases. (Left) Model metadata including key metrics; (Right) Models in a W&B project sorted by the <code>best_matthews_corrcoef</code> metadata associated with their respective <em>best models</em></p>
</figcaption>
</figure>
<h2 id=wrapping-up>Wrapping up<a hidden class=anchor aria-hidden=true href=#wrapping-up>#</a></h2>
<p>So, what have we learned in this article?</p>
<ul>
<li>Only saving the <em>best model</em> during training is efficient and encourages good practices</li>
<li>The metadata, including key metrics associated with the <em>best model</em>, is almost as important as the model artifact itself</li>
<li>Using fastai and Weights & Biases, saving and documenting the <em>best model</em> can be done automatically for you. Two custom callback functions were described to make this process even better (<a href=https://gist.github.com/nicjac/b363d2454ea253570a54e5e178e7666a>SaveModelCallback</a> and <a href=https://gist.github.com/nicjac/9efb56cccd57f9c84910f02ccabf6fac>WandbCallback</a>).</li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://nicjac.dev/tags/deep-learning/>Deep Learning</a></li>
<li><a href=https://nicjac.dev/tags/ai/>AI</a></li>
<li><a href=https://nicjac.dev/tags/weightsbiases/>Weights&Biases</a></li>
<li><a href=https://nicjac.dev/tags/fastai/>fastai</a></li>
<li><a href=https://nicjac.dev/tags/good-practices/>good practices</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://nicjac.dev/posts/how-to-build-machine-learning-demo-in-2022/>
<span class=title>« Prev Page</span>
<br>
<span>How to Build a Machine Learning Demo in 2022</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on twitter" href="https://twitter.com/intent/tweet/?text=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models&url=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f&hashtags=DeepLearning%2cAI%2cWeights%26Biases%2cfastai%2cgoodpractices"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f&title=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models&summary=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models&source=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f&title=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on whatsapp" href="https://api.whatsapp.com/send?text=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models%20-%20https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Good practices for neural network training: identify, save, and document best models on telegram" href="https://telegram.me/share/url?text=Good%20practices%20for%20neural%20network%20training%3a%20identify%2c%20save%2c%20and%20document%20best%20models&url=https%3a%2f%2fnicjac.dev%2fposts%2fidentify-best-model%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://nicjac.dev/>Reasonable AI</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>